{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in /opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages (from nltk>=3.8->textblob) (2022.3.15)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages (from nltk>=3.8->textblob) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages (from nltk>=3.8->textblob) (4.64.0)\n",
      "Requirement already satisfied: click in /opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages (from nltk>=3.8->textblob) (8.0.4)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict, Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "!pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/news_cleaned.csv')\n",
    "#df['tokens'] = df['tokens'].str.replace(\"'\", \"\")\n",
    "#df['tokens_no_climate'] = df['tokens_no_climate'].str.replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90863"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis with Textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textblob polarity scoring is between [-1.0 to 1.0] where -1.0 indicates negative sentiment and 1.0 indicates positive sentiment.\n",
    "\n",
    "Textblob subjectivity scoring is between [0.0 to 1.0], where 0.0 is very objective, and 1.0 is very subjective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['polarity'] = df['snippet'].apply(lambda x: TextBlob(x).polarity)\n",
    "df['subjectivity'] = df['snippet'].apply(lambda x: TextBlob(x).subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchdatetime</th>\n",
       "      <th>station</th>\n",
       "      <th>snippet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>snippet_no_climate</th>\n",
       "      <th>tokens_no_climate</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77120</th>\n",
       "      <td>2019-04-14 19:22:49</td>\n",
       "      <td>FOXNEWS</td>\n",
       "      <td>they wanted to be more moderate rather than mo...</td>\n",
       "      <td>['wanted', 'moderate', 'rather', 'liberal', 'p...</td>\n",
       "      <td>they wanted to be more moderate rather than mo...</td>\n",
       "      <td>['wanted', 'moderate', 'rather', 'liberal', 'p...</td>\n",
       "      <td>0.089123</td>\n",
       "      <td>0.448864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11141</th>\n",
       "      <td>2019-08-14 14:56:20</td>\n",
       "      <td>BBCNEWS</td>\n",
       "      <td>-- mitigate. -- mitigate. that is of course th...</td>\n",
       "      <td>['mitigate', 'mitigate', 'course', 'big', 'que...</td>\n",
       "      <td>-- mitigate. -- mitigate. that is of course th...</td>\n",
       "      <td>['mitigate', 'mitigate', 'course', 'big', 'que...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59882</th>\n",
       "      <td>2013-08-16 18:44:26</td>\n",
       "      <td>FOXNEWS</td>\n",
       "      <td>end run around congress. the agency moving to ...</td>\n",
       "      <td>['end', 'run', 'around', 'congress', 'agency',...</td>\n",
       "      <td>end run around congress. the agency moving to ...</td>\n",
       "      <td>['end', 'run', 'around', 'congress', 'agency',...</td>\n",
       "      <td>0.278788</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45280</th>\n",
       "      <td>2015-11-09 19:44:45</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>totally is. anyway, no. it is a missile. presi...</td>\n",
       "      <td>['totally', 'anyway', 'missile', 'president', ...</td>\n",
       "      <td>totally is. anyway, no. it is a missile. presi...</td>\n",
       "      <td>['totally', 'anyway', 'missile', 'president', ...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32942</th>\n",
       "      <td>2015-09-21 21:19:48</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>popular pope. at the same time, he is somewhat...</td>\n",
       "      <td>['popular', 'pope', 'time', 'somewhat', 'contr...</td>\n",
       "      <td>popular pope. at the same time, he is somewhat...</td>\n",
       "      <td>['popular', 'pope', 'time', 'somewhat', 'contr...</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.595000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             matchdatetime  station  \\\n",
       "77120  2019-04-14 19:22:49  FOXNEWS   \n",
       "11141  2019-08-14 14:56:20  BBCNEWS   \n",
       "59882  2013-08-16 18:44:26  FOXNEWS   \n",
       "45280  2015-11-09 19:44:45    MSNBC   \n",
       "32942  2015-09-21 21:19:48    MSNBC   \n",
       "\n",
       "                                                 snippet  \\\n",
       "77120  they wanted to be more moderate rather than mo...   \n",
       "11141  -- mitigate. -- mitigate. that is of course th...   \n",
       "59882  end run around congress. the agency moving to ...   \n",
       "45280  totally is. anyway, no. it is a missile. presi...   \n",
       "32942  popular pope. at the same time, he is somewhat...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "77120  ['wanted', 'moderate', 'rather', 'liberal', 'p...   \n",
       "11141  ['mitigate', 'mitigate', 'course', 'big', 'que...   \n",
       "59882  ['end', 'run', 'around', 'congress', 'agency',...   \n",
       "45280  ['totally', 'anyway', 'missile', 'president', ...   \n",
       "32942  ['popular', 'pope', 'time', 'somewhat', 'contr...   \n",
       "\n",
       "                                      snippet_no_climate  \\\n",
       "77120  they wanted to be more moderate rather than mo...   \n",
       "11141  -- mitigate. -- mitigate. that is of course th...   \n",
       "59882  end run around congress. the agency moving to ...   \n",
       "45280  totally is. anyway, no. it is a missile. presi...   \n",
       "32942  popular pope. at the same time, he is somewhat...   \n",
       "\n",
       "                                       tokens_no_climate  polarity  \\\n",
       "77120  ['wanted', 'moderate', 'rather', 'liberal', 'p...  0.089123   \n",
       "11141  ['mitigate', 'mitigate', 'course', 'big', 'que...  0.100000   \n",
       "59882  ['end', 'run', 'around', 'congress', 'agency',...  0.278788   \n",
       "45280  ['totally', 'anyway', 'missile', 'president', ...  0.083333   \n",
       "32942  ['popular', 'pope', 'time', 'somewhat', 'contr...  0.330000   \n",
       "\n",
       "       subjectivity  \n",
       "77120      0.448864  \n",
       "11141      0.150000  \n",
       "59882      0.533333  \n",
       "45280      0.361111  \n",
       "32942      0.595000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Average Polarity  Average Subjectivity\n",
      "station                                        \n",
      "BBCNEWS          0.088726              0.395586\n",
      "CNN              0.097142              0.397627\n",
      "FOXNEWS          0.075069              0.369970\n",
      "MSNBC            0.099143              0.396647\n"
     ]
    }
   ],
   "source": [
    "# calculate average polarity and subjectivity for each station\n",
    "station_stats = df.groupby('station').agg({'polarity': 'mean', 'subjectivity': 'mean'})\n",
    "\n",
    "# rename column\n",
    "station_stats.columns = ['Average Polarity', 'Average Subjectivity']\n",
    "print(station_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is overall positive sentiment for all four stations, with little to none significant differences between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an overall objective reporting of climate change for all news stations with little significant differences between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environmental catastrophe in another part of the world. so far, administration officials are not backing away from nuclear. which they said will reduce emissions and prevent climate\n",
      "strict greenhouse gas reduction law. prop 23 would suspend that law and that, of course, would be awesome for companies that make a lot of money by making a lot of pollution. 97% of the funding for prop 23 so far comes from oil and chemical companies, including a\n",
      "and state chapters of the naacp. the letters urged perriello to vote against climate change legislation. the letters were fake. tea party groups camped out mr. perriello's virginia office, one\n",
      "targeting climate change, is there a bit of hypocrisy of it? i disagree with it. i think you find it funny. he made hundred million. he is 3500 votes of being\n",
      "producing countries? yes, it is. so you al gore are doing business with this country. [ laughter ] that's enabling your ultimate foe, climate change? i think i understand what you are getting at. [ laughter ] but i disagree with it.\n",
      "our dependence on petroleum producing countries? yes, it is. so you al gore are doing business with this country. [ laughter ] that's enabling your ultimate foe, climate change? i think i understand what you are getting at. [ laughter ] but i disagree with it.\n",
      "world in terms of what is going on? because you can, we could argue about climate change and taub about the environment on a important issue no doubt but is that a priority issue when talking finite space of the inaugural address is that\n",
      "been dishonourably discharged. matthew scully-hicks has been found guilty of murdering his 18-month-old baby just two weeks after formally adopting her. as the un climate change conference\n",
      "about his wife. this year is likely to be one of the warmest on record, that's according to the world metrelogical organisation. its report was released as the un climate change conference began\n",
      "eating lunch and using his mobile phone. highways england, which funded the vehicle, said it would continue to use the hgv to catch offenders. this year is likely to be one of the warmest on record, that's according to the world meteorological organisation. its report was released as the un climate change conference began\n"
     ]
    }
   ],
   "source": [
    "# see 10 most subjective snippets\n",
    "s= df.nlargest(10, 'subjectivity')[['snippet','station']].index\n",
    "for index in s:\n",
    "    print(df.loc[index, 'snippet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by 2050, countries like that might not exist. closer to home, things like wildfires, devastating hurricanes, food shortages, migrations, they're all a host of awful things associated with climate change. we're already seeing the beginnings of this now. and this report just underscores\n",
      "report warns of devastating effects from climate change. president trump suggested that he doesn't believe it, what's your response to the president? look, the climate den\n",
      "we're going to have to build shelters so people can escape when these terrible fires get out of hand. and yes, we're going to have to deal with climate change. all of that. reporter: meanwhile, 145 evacuees and workers in shelters around butte county are suffering from norovirus.\n",
      "published scientific literature. so what this report will tell us is that we are seeing the impact of climate change on our coastlines here in the united states, in terms of devastating superstorms. you add a foot of sea level rise and we could see six feet to\n",
      "impact on our economy. so as a policymaker i refuse to support initiatives that l. do nothing to impact the environment or the climate but have devastating impact on our economy. so i understand if you had sufficient information presented to you to show that climate change exists\n",
      "climate change. a country like colombia, that is especially vulnerable, because of our geography, our location. what happened, this tragedy, it is a demonstration that climate change has terrible effects. the intensity\n",
      "he is weakening america, he is losing jobs, he's exposing americans to worst climate change. americans to worst climate change. we'll have the details and the reaction to the president's announcement. also tonight:\n",
      "that you are the worst person to be appointed to thatjob. appointed to that job. one of the things i will say is, when it comes to climate change, before i was ever an mp and indeed before david cameron became leader of the conservative party and put the\n",
      "i don't watch comedies alone. a side of rice that no one ordered. what insane comment susan rice said about climate change after the break. i will give conservatives\n",
      "videos from rachel maddow, as well as msnbc.com. he shared a cnn video of an anti-trump climate change rally in which the shooter revealed he was among the thousands of protesters here in washington. in one particularly nasty post on facebook, hodgkinson\n"
     ]
    }
   ],
   "source": [
    "# see 10 most negative snippets\n",
    "s= df.nsmallest(10, 'polarity')[['snippet','station']].index\n",
    "for index in s:\n",
    "    print(df.loc[index, 'snippet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issues, or pressing concerns - whether it be climate change, animal exploitation or refugees. at the forefront of films addressing the refugee crisis was 80-year-old legendary actress, vanessa redgrave,\n",
      "some scientists have called climate change the greatest threat that humanity changes. president trump's defense secretar james mattis called it a challenge to national security. the president said he would make\n",
      "that is not all. causation is the republican resolution that climate change is happening and we need to find a solution. while she has had an impressive start in congress, she does not plan to be there forever. i do think institutionally congress benefits from having a\n",
      "candidates. by the way, in massachusetts they say the shape of the field determines the winner. here's the people that look like they may run against her. maybe ed markey, very impressive senior who did all this mark pushing the climate change and\n",
      "truly greatest weapons. but the speech had nothing to say about climate change,\n",
      "is national security. gdp growth, which is way ahead of schedule under my administration, will be one of america's truly greatest weapons. but the speech had nothing to say about climate change,\n",
      "schedule under my administration, will be one of america's truly greatest weapons. but the speech had nothing to say about climate change, something that barack obama had deemed a threat to national\n",
      "biofuel. brazil has the world's largest fleet of flex fuel cars, meaning they can run on both petrol and ethanol. for years, the industry here thought biofuels were the best way to reduce greenhouse gas\n",
      "biofuel. brazil has the world 's largest amount of flux fuels which means a command petrol and ethanol. for yea rs for years the industry thought biofuels with the best way to reduce greenhouse gas emissions but now the world seems to be going electric.\n",
      "matt mcgrath reports. for decades, the disease have been oui' for decades, the disease have been our best friend in the fight against climate change, soaking up carbon dioxide and access heat. the scale of verbal warming means we have gone\n"
     ]
    }
   ],
   "source": [
    "# see 10 most positive snippets\n",
    "s = df.nlargest(10, 'polarity')[['snippet','station']].index\n",
    "for index in s:\n",
    "    print(df.loc[index, 'snippet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/sentiment_analysis.ipynb Cell 14\u001b[0m line \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/sentiment_analysis.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m count_vectorizer \u001b[39m=\u001b[39m CountVectorizer(stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m, min_df\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, max_df\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/sentiment_analysis.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m count_vectorizer\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/sentiment_analysis.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m count_vectors \u001b[39m=\u001b[39m count_vectorizer\u001b[39m.\u001b[39;49mfit_transform(combined_tokens)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/sentiment_analysis.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m count_vectors\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m             )\n\u001b[1;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1228\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1226\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1228\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1229\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1230\u001b[0m         )\n\u001b[1;32m   1232\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "combined_tokens=[' '.join(sublist) for sublist in df['tokens']]\n",
    "count_vectorizer = CountVectorizer(stop_words='english', min_df=3, max_df=0.7)\n",
    "count_vectorizer\n",
    "count_vectors = count_vectorizer.fit_transform(combined_tokens)\n",
    "count_vectors\n",
    "#count_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/data_import copy.ipynb Cell 10\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/data_import%20copy.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# TF-IDF Vectorizer\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/data_import%20copy.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tfidf_vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m, min_df\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, max_df\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/data_import%20copy.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tfidf_vectors \u001b[39m=\u001b[39m tfidf_vectorizer\u001b[39m.\u001b[39;49mfit_transform(combined_tokens)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/data_import%20copy.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tfidf_vectors\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2078\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m   2072\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2073\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[1;32m   2074\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[1;32m   2075\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[1;32m   2076\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[1;32m   2077\u001b[0m )\n\u001b[0;32m-> 2078\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   2079\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   2080\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2081\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m             )\n\u001b[1;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1228\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1226\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1228\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1229\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1230\u001b[0m         )\n\u001b[1;32m   1232\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.7)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(combined_tokens)\n",
    "tfidf_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/data_import.ipynb Cell 17\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/data_import.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Fitting LDA Model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/data_import.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lda_model \u001b[39m=\u001b[39m LatentDirichletAllocation(n_components\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m314\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/data_import.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m W_lda_matrix \u001b[39m=\u001b[39m lda_model\u001b[39m.\u001b[39;49mfit_transform(count_vectors)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/data_import.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m H_lda_matrix \u001b[39m=\u001b[39m lda_model\u001b[39m.\u001b[39mcomponents_\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/viviando/Desktop/MSADS/ads599_capstone/climate-lang-processing/data_import.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Display LDA Model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages/sklearn/base.py:867\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    868\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:640\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_em_step(\n\u001b[1;32m    633\u001b[0m             X[idx_slice, :],\n\u001b[1;32m    634\u001b[0m             total_samples\u001b[39m=\u001b[39mn_samples,\n\u001b[1;32m    635\u001b[0m             batch_update\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    636\u001b[0m             parallel\u001b[39m=\u001b[39mparallel,\n\u001b[1;32m    637\u001b[0m         )\n\u001b[1;32m    638\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    639\u001b[0m     \u001b[39m# batch update\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_em_step(\n\u001b[1;32m    641\u001b[0m         X, total_samples\u001b[39m=\u001b[39;49mn_samples, batch_update\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, parallel\u001b[39m=\u001b[39;49mparallel\n\u001b[1;32m    642\u001b[0m     )\n\u001b[1;32m    644\u001b[0m \u001b[39m# check perplexity\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[39mif\u001b[39;00m evaluate_every \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m evaluate_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:503\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._em_step\u001b[0;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[39m\"\"\"EM update for 1 iteration.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \n\u001b[1;32m    478\u001b[0m \u001b[39mupdate `_component` by batch VB or online VB.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39m    Unnormalized document topic distribution.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m# E-step\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m _, suff_stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_e_step(\n\u001b[1;32m    504\u001b[0m     X, cal_sstats\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, random_init\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, parallel\u001b[39m=\u001b[39;49mparallel\n\u001b[1;32m    505\u001b[0m )\n\u001b[1;32m    507\u001b[0m \u001b[39m# M-step\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m batch_update:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:446\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._e_step\u001b[0;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39mif\u001b[39;00m parallel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m--> 446\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    447\u001b[0m     delayed(_update_doc_distribution)(\n\u001b[1;32m    448\u001b[0m         X[idx_slice, :],\n\u001b[1;32m    449\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexp_dirichlet_component_,\n\u001b[1;32m    450\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdoc_topic_prior_,\n\u001b[1;32m    451\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_doc_update_iter,\n\u001b[1;32m    452\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean_change_tol,\n\u001b[1;32m    453\u001b[0m         cal_sstats,\n\u001b[1;32m    454\u001b[0m         random_state,\n\u001b[1;32m    455\u001b[0m     )\n\u001b[1;32m    456\u001b[0m     \u001b[39mfor\u001b[39;49;00m idx_slice \u001b[39min\u001b[39;49;00m gen_even_slices(X\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], n_jobs)\n\u001b[1;32m    457\u001b[0m )\n\u001b[1;32m    459\u001b[0m \u001b[39m# merge result\u001b[39;00m\n\u001b[1;32m    460\u001b[0m doc_topics, sstats_list \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mresults)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ADS500B/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:122\u001b[0m, in \u001b[0;36m_update_doc_distribution\u001b[0;34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[1;32m    118\u001b[0m last_d \u001b[39m=\u001b[39m doc_topic_d\n\u001b[1;32m    120\u001b[0m \u001b[39m# The optimal phi_{dwk} is proportional to\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m# exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m norm_phi \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(exp_doc_topic_d, exp_topic_word_d) \u001b[39m+\u001b[39m EPS\n\u001b[1;32m    124\u001b[0m doc_topic_d \u001b[39m=\u001b[39m exp_doc_topic_d \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mdot(cnts \u001b[39m/\u001b[39m norm_phi, exp_topic_word_d\u001b[39m.\u001b[39mT)\n\u001b[1;32m    125\u001b[0m \u001b[39m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## takes >25 minutes\n",
    "# Fitting LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=314)\n",
    "W_lda_matrix = lda_model.fit_transform(count_vectors)\n",
    "H_lda_matrix = lda_model.components_\n",
    "\n",
    "# Display LDA Model\n",
    "display_topics(lda_model, count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.lda_model.prepare(lda_model, count_vectors, count_vectorizer, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA w/ Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gensim_tokens = df['tokens']\n",
    "\n",
    "# initialize Gensim dictionary \n",
    "dict_gensim = Dictionary(gensim_tokens)\n",
    "\n",
    "# filter for words that appear in: at least 5 but not more than 70% of all snippets\n",
    "dict_gensim.filter_extremes(no_below=5, no_above=0.7)\n",
    "\n",
    "# calculate bag of words matrix\n",
    "bow_gensim = [dict_gensim.doc2bow(token) for token in gensim_tokens]\n",
    "\n",
    "# perform TF-IDF transformation\n",
    "\n",
    "tfidf_gensim = TfidfModel(bow_gensim)\n",
    "vectors_gensim = tfidf_gensim[bow_gensim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using LDA with Gensim\n",
    "lda_gensim = LdaModel (corpus = bow_gensim,\n",
    "                       id2word = dict_gensim,\n",
    "                       chunksize = 2000,\n",
    "                       alpha = 'auto',\n",
    "                       eta = 'auto',\n",
    "                       iterations = 400,\n",
    "                       num_topics = 5,\n",
    "                       passes = 20,\n",
    "                       eval_every = None,\n",
    "                       random_state = 509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: 0.024*\"world\" + 0.018*\"greenhouse\" + 0.017*\"new\" + 0.017*\"emissions\" + 0.014*\"carbon\" + 0.013*\"gas\" + 0.011*\"government\" + 0.011*\"action\" + 0.010*\"environment\" + 0.010*\"news\"\n",
      "Topic #2: 0.017*\"weather\" + 0.011*\"facing\" + 0.009*\"record\" + 0.008*\"air\" + 0.008*\"water\" + 0.008*\"seen\" + 0.008*\"extreme\" + 0.008*\"across\" + 0.007*\"become\" + 0.006*\"finds\"\n",
      "Topic #3: 0.023*\"president\" + 0.021*\"us\" + 0.015*\"said\" + 0.014*\"trump\" + 0.010*\"first\" + 0.010*\"today\" + 0.009*\"next\" + 0.008*\"two\" + 0.008*\"crisis\" + 0.008*\"hes\"\n",
      "Topic #4: 0.025*\"global\" + 0.023*\"warming\" + 0.020*\"people\" + 0.016*\"think\" + 0.014*\"going\" + 0.013*\"like\" + 0.013*\"one\" + 0.012*\"say\" + 0.010*\"get\" + 0.010*\"would\"\n",
      "Topic #5: 0.024*\"years\" + 0.018*\"year\" + 0.017*\"scientists\" + 0.017*\"could\" + 0.012*\"says\" + 0.012*\"report\" + 0.010*\"impact\" + 0.009*\"temperatures\" + 0.008*\"planet\" + 0.008*\"global\"\n"
     ]
    }
   ],
   "source": [
    "# see word distribution of topics\n",
    "#display_topics_gensim(lda_gensim)\n",
    "lda_gensim_topics = lda_gensim.show_topics(num_topics=5, num_words=10)\n",
    "\n",
    "# Display the topics\n",
    "for topic_idx, topic in lda_gensim_topics:\n",
    "    print(f\"Topic #{topic_idx + 1}: {topic}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS500B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
